# The Google Colab Notebook was downloaded as a python file due to Github file size constraints
"""Kathakali.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N4t4eQ4fuZoipsha-c6PMTdfzD0yVNil
"""

#@title Preparing our environment  { display-mode: "form" }

import cv2
import dlib
import pickle
import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import itertools

import urllib.request

from sklearn import metrics
from scipy.spatial import distance
from sklearn.metrics import accuracy_score
from matplotlib import pyplot as plt
from tqdm import tqdm,tqdm_pandas
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA

import re
import keras

from keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam, SGD
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from keras.losses import categorical_crossentropy
from keras.regularizers import l2
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import load_model

# grab tools from our tensorflow and keras toolboxes!
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import optimizers

warnings.filterwarnings("ignore")

def model_to_string(model):
    import re
    stringlist = []
    model.summary(print_fn=lambda x: stringlist.append(x))
    sms = "\n".join(stringlist)
    sms = re.sub('_\d\d\d','', sms)
    sms = re.sub('_\d\d','', sms)
    sms = re.sub('_\d','', sms)
    return sms

!wget -q --show-progress "https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/fer2013_5.csv"

!wget -q --show-progress "https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/shape_predictor_68_face_landmarks.dat"

###Getting the Xpure loaded
!wget -q --show-progress "https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/pureX.npy"

###Getting the Xdata loaded
!wget -q --show-progress -O ./dataX.npy "https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/dataX_edited.npy"

!wget -q --show-progress "https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/dataY.npy"

print ("Data Downloaded!")

'''
Plots the confusion Matrix and saves it
'''
def plot_confusion_matrix(y_true,y_predicted):
  cm = metrics.confusion_matrix(y_true, y_predicted)
  print ("Plotting the Confusion Matrix")
  labels = list(label_map.values())
  df_cm = pd.DataFrame(cm,index = labels,columns = labels)
  fig = plt.figure()
  res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')
  plt.yticks([0.5,1.5,2.5,3.5,4.5], labels,va='center')
  plt.title('Confusion Matrix - TestData')
  plt.ylabel('True label')
  plt.xlabel('Predicted label')

  plt.show()
  plt.close()

def plot_graphs(history, best):

  plt.figure(figsize=[10,4])
  # summarize history for accuracy
  plt.subplot(121)
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  plt.title('model accuracy across training\n best accuracy of %.02f'%best[1])
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')

  # summarize history for loss
  plt.subplot(122)
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('model loss across training\n best loss of %.02f'%best[0])
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()

#Integer to Label Mapping
label_map = {"0":"ANGRY","1":"HAPPY","2":"SAD","3":"SURPRISE","4":"NEUTRAL"}


#Load the 68 face Landmark file
predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')
"""
Returns facial landmarks for the given input image path
"""
def get_landmarks(image):

  rects = [dlib.rectangle(left=1, top=1, right=47, bottom=47)]

  landmarks = [(p.x, p.y) for p in predictor(image, rects[0]).parts()]
  return image,landmarks

"""
Display image with its Facial Landmarks
"""
def image_landmarks(image,face_landmarks):
  """
  :type image_path : str
  :type face_landmarks : list of tuples where each tuple represents
                     the x and y co-ordinates of facial keypoints
  :rtype : None
  """
  radius = -4
  circle_thickness = 1
  image_copy = image.copy()
  for (x, y) in face_landmarks:
    cv2.circle(image_copy, (x, y), circle_thickness, (255,0,0), radius)

  plt.imshow(image_copy, interpolation='nearest')
  plt.show()

"""
Computes euclidean distance between 68 Landmark Points for our features
e_dist is a list of features that will go into our model.
Each feature is a distance between two landmark points, and every pair of points
must have a feature.
"""

def landmarks_edist(face_landmarks):
    e_dist = []
    for i,j  in itertools.combinations(range(68), 2):
      e_dist.append(distance.euclidean(face_landmarks[i],face_landmarks[j]))
    return e_dist

def compare_learning(mlp, lm, cnn, vgg): # there's one model missing: MLP from pixels

  # summarize history for accuracy
  plt.plot(vgg.history['val_accuracy'],)
  plt.plot(cnn.history['val_accuracy'])
  plt.plot(mlp.history['val_accuracy'],)
  plt.plot(lm.history['val_accuracy'])
  plt.ylabel('validitation accuracy')
  plt.xlabel('epoch')
  plt.legend(['cnn_transfer', 'cnn_scratch', 'mlp_pixels', 'mlp_landmarks'], bbox_to_anchor=[1,1])
  plt.xticks(range(0, epochs+1, 5), range(0, epochs+1, 5))
  plt.show()

"""# Using Euclidean Distance between Facial Features to Identify Facial Emotion"""

# dlib's pretrained face detector model
frontalface_detector = dlib.get_frontal_face_detector()

#@title Creating Bounding Boxes

def rect_to_bb(rect):
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y
    return (x, y, w, h)

#@title Face Detection on a given image

def detect_face_using_url(image_url):
  try:

    url_response = urllib.request.urlopen(image_url)
    img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)
    image = cv2.imdecode(img_array, -1)

  except Exception as e:
    return "Incorrect URL!"

  rects = frontalface_detector(image, 1)

  if len(rects) < 1:
    return "No Face Detected"

  for (i, rect) in enumerate(rects):
    (x, y, w, h) = rect_to_bb(rect)
    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)
  plt.imshow(image, interpolation='nearest')
  plt.axis('off')
  plt.show()

def detect_face_from_gdrive(image_path):
    try:
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError("Image could not be loaded. Check the file path!")

        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    except Exception as e:
        return f"Error: {e}"

    # Detect faces in the image
    rects = frontalface_detector(gray, 1)

    # Handle cases with no faces detected
    if len(rects) < 1:
        return "No Face Detected"

    # Draw rectangles around detected faces
    for (i, rect) in enumerate(rects):
        (x, y, w, h) = rect_to_bb(rect)
        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)

    # Display the image with detected faces
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.show()

# https://i.pinimg.com/236x/27/28/0e/27280ee28567c1e20c119f74981ee5c4--black-freckles-freckles-makeup.jpg

path = '/content/drive/MyDrive/data/photo_face_detection.png'
detect_face_from_gdrive(path)

frontalface_detector = dlib.get_frontal_face_detector()

landmark_predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')

#@title Retrieving Facial Landmarks from the Image

def get_landmarks(image_path):

    try:
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError("Image could not be loaded. Check the file path!")

        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    except Exception as e:
        return f"Error: {e}"

    faces = frontalface_detector(gray, 1)
    if len(faces):
      landmarks = [(p.x, p.y) for p in landmark_predictor(gray, faces[0]).parts()]
    else:
      return None,None

    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB),landmarks

#@title Visualizing landmarks (function)

def plot_image_landmarks(image,face_landmarks):

  radius = -1
  circle_thickness = 15
  image_copy = image.copy()
  for (x, y) in face_landmarks:
    cv2.circle(image_copy, (x, y), circle_thickness, (255,0,0), radius)

  plt.imshow(image_copy, interpolation='nearest')
  plt.axis('off')
  plt.show()

image,landmarks= get_landmarks('/content/drive/MyDrive/data/photo_face_detection.png')

if landmarks:
  plot_image_landmarks(image,landmarks)
else:
  print ("No Landmarks Detected")

#@title Run (and eventually edit) this cell to visualize the features we've extracted

def show_indices(landmarks, i_index):

  plt.scatter(x=[landmarks[i][0] for i in range(len(landmarks)//2, len(landmarks))],
              y=[-landmarks[i][1] for i in range(len(landmarks)//2, len(landmarks))], s=50, alpha=.5, color='blue', label='second half of indices')

  plt.scatter(x=[landmarks[i][0] for i in range(len(landmarks)//2)],
              y=[-landmarks[i][1] for i in range(len(landmarks)//2)], color='red', alpha=.5, label='first half of indices')

  # what should X and Y be to visualize the feature at i_index?
  x = landmarks[i_index][0]
  y = -landmarks[i_index][1]
  plt.scatter(x=x, y=y,
             color='purple', s=100, marker='x', label='feature at index %d'%i_index)

  plt.scatter(x, y, color='red', alpha=.5, label='selected indices')

  plt.axis('off');
  plt.legend(bbox_to_anchor=[1,1]);
  plt.title('Visualizing the features extracted from the image',y =1.2);

show_index = 67
show_indices(landmarks, show_index)

np.array(landmarks).shape

landmark_indices = {'eyes':(36,47),
                    "nose":(27,35),
                    "mouth":(48,67),
                    "jawline":(0,17),
                    "eyebrow":(18,27)}

# Display images with individual detection of face parts

eye_points = np.array([36,47])
nose_points = np.array([27,35])
mouth_points = np.array([48,67])
jawline_points = np.array([0,17])
eyebrow_points = np.array([18,27])
selected_landmarks = landmarks[eye_points[0]:eye_points[1]+1]
print(selected_landmarks)
plot_image_landmarks(image,selected_landmarks)

def euclidean_distance(p1,p2):
  x1, y1 = p1
  x2, y2 = p2
  xdistance = float(x2) - float(x1)
  ydistance = float(y2) - float(y1)
  distance = math.sqrt((xdistance)**2 + (ydistance)**2)
  return distance

import math
def classify_images(image1_path,image2_path):

  image1,image1_landmarks = get_landmarks(image1_path)
  image2,image2_landmarks = get_landmarks(image2_path)

  plot_image_landmarks(image1, image1_landmarks)
  plot_image_landmarks(image2, image2_landmarks)

  eye_points = np.array([36,47])
  image1_landmarks = image1_landmarks[eye_points[0]:eye_points[1]+1]
  image2_landmarks = image2_landmarks[eye_points[0]:eye_points[1]+1]
  print(image1_landmarks)

  d1, d2 = 0.0,0.0
  for i in range(len(image1_landmarks)-1):
    d1 = d1 + euclidean_distance(image1_landmarks[i], image1_landmarks[i+1])
  for i in range(len(image2_landmarks)-1):
    d2 += euclidean_distance(image2_landmarks[i], image2_landmarks[i+1])
  print(d1)
  print(d2)

image1_path = "/content/drive/MyDrive/data/photo_face_detection.png"
image2_path = "/content/drive/MyDrive/data/photo_eyes_closed.png"
classify_images(image1_path, image2_path)

"""# Method 1: Machine Learning using KNNs, Logistic Regression with Euclidean Distance as Input

Logistic Regression : 49.3%
"""

label_map = {0:"ANGRY",1:"HAPPY",2:"SAD",3:"SURPRISE",4:"NEUTRAL"}

df = pd.read_csv("fer2013_5.csv")
df.head()

emotion_labels = [label_map[i] for i in label_map.keys()]

emotion_counts = [np.sum(df["emotion"] == i) for i in range(len(label_map))]

[plt.bar(x = emotion_labels[i], height = emotion_counts[i] ) for i in range(len(emotion_labels))]

plt.xlabel('EMOTION LABEL')
plt.ylabel('N OBSERVSATIONS')
plt.title('A balanced distribution of emotions in our data set', y=1.05);

#@title Extraction of Facial Landmarks

predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')
"""
Returns facial landmarks for the given input image path
"""
def get_landmarks(image):
  rects = [dlib.rectangle(left=1, top=1, right=47, bottom=47)]

  #Read Image using OpenCV
  #image = cv2.imread(image_path)
  #Detect the Faces within the image
  landmarks = [(p.x, p.y) for p in predictor(image, rects[0]).parts()]
  return image,landmarks

def plot_image_landmarks(image,face_landmarks):
  radius = -2
  circle_thickness = 1
  image_copy = image.copy()
  for (x, y) in face_landmarks:
    cv2.circle(image_copy, (x, y), circle_thickness, (255,0,0), radius)

  plt.imshow(image_copy, interpolation='nearest', cmap='Greys_r')
  plt.xticks([]); plt.yticks([])
  plt.show()


def get_pixels_image(img_pixels,plt_flag):

  width = 48
  height = 48

  image = np.fromstring(img_pixels, dtype=np.uint8, sep=" ").reshape((height, width))

  if plt_flag:
      plt.imshow(image, interpolation='nearest', cmap="Greys_r")
      plt.xticks([]); plt.yticks([])
      plt.show()


  return image

#@title Printing random sample from dataset
i_index = np.random.randint(len(df))
image_pixels = df['pixels'][i_index]
image = get_pixels_image(image_pixels, True)

label_index = df["emotion"][i_index]
print(f"label: " + emotion_labels[label_index])

#Extract the Facial Landmarks
image,facial_landmarks = get_landmarks(image)

#Display the Facial Landmarks on the Image
plot_image_landmarks(image,facial_landmarks)

"""
Computes euclidean distance between 68 Landmark Points for our features
e_dist is a list of features that will go into our model.
Each feature is a distance between two landmark points, and every pair of points
must have a feature.
Scipy Library has readily available fuction to compute euclidean distance. You can
compute the distance using distance.euclidean(point1,point2)
point1,point2 :2D points
"""
def get_all_landmarks_euclid_dist(face_landmarks):
    e_dist = []
    for i, j in itertools.combinations(range(len(facial_landmarks)), 2):
      e_dist.append(distance.euclidean(face_landmarks[i], face_landmarks[j]))
    return e_dist


get_all_landmarks_euclid_dist(facial_landmarks)

#@title Function that preprocesses the data to extract distances between all points

def preprocess_data(df):

  X = [] #the 2278 euclidean distance values for each image stored as one array inside the array
  Y = [] #the labels
  X_pixels = [] #the pixel values of each image

  n_pixels = 2304

  for index, row in (df.iterrows()):

      if index%1000 == 0:
        print (index, "Datapoints Processed")

      try:
          image = get_pixels_image(row['pixels'],0)
          X_pixels.append(image.ravel())
          image = cv2.GaussianBlur(image,(5,5),0)

          _,face_landmarks = get_landmarks(image)
          X.append(get_all_landmarks_euclid_dist(face_landmarks)) # Using our feature function!
          Y.append(row['emotion'])

      except Exception as e:
          print ("An error occured:",e)

  np.save("pureX", X_pixels)
  np.save("dataX", X)
  np.save("dataY", Y)

  return np.array(X_pixels),np.array(X),np.array(Y)

# set to True to preload data
preload = True

if preload:

  dataX = np.load('./dataX.npy')
  dataY = np.load('./dataY.npy', allow_pickle=True)

else:

  pureX, dataX, dataY = preprocess_data(df)

#@title 90-10 train test split
X_train, X_test, y_train, y_test = train_test_split(dataX, dataY, test_size=0.1, random_state=42,stratify =dataY)

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train,y_train)
model.score(X_test,y_test)

#@title Standardizing the data
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

#@title Principal Component Analysis for Input Feature Dimensionality Reduction
pca = PCA(.95)
pca.fit(X_train)

X_train = pca.transform(X_train)
X_test= pca.transform(X_test)

X_train.shape

#@title Training Machine Learning Models

#K Nearest Neighbors Classifier
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 20)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print('K Nearest Neighbors Classifier')
print("Accuracy: ", metrics.accuracy_score(y_test, y_pred))

for i in range(20, 40):
  knn2 = KNeighborsClassifier(n_neighbors = i)
  knn2.fit(X_train, y_train)
  y_pred2 = knn2.predict(X_test)
  print('Accuracy for ', i ,' n_neighbors: ', metrics.accuracy_score(y_test, y_pred2))

#Logistic Regression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train, y_train)
y_pred3 = lr.predict(X_test)
print('Logistic Regression')
print('Accuracy: ', metrics.accuracy_score(y_test, y_pred3))

#Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
print('Random Forest Classifer')
for i in range(100, 250, 10):
  Rf = RandomForestClassifier(i)
  Rf.fit(X_train, y_train)
  y_pred4 = Rf.predict(X_test)
  print('Accuracy for ', i, 'n_estimators : ', metrics.accuracy_score(y_test, y_pred4))

"""## Results
The best Machine Learning Model was Random Forest Classifier with 230 estimators at 55.7% accuracy.

K Nearest Neighbor model had an accuracy less than 50%.

Logistic Regression Accuracy = 50.3%
"""

Rf = RandomForestClassifier(230)
Rf.fit(X_train, y_train)
y_pred5 = Rf.predict(X_test)

def plot_confusion_matrix(y_true,y_predicted):
  cm = metrics.confusion_matrix(y_true, y_predicted)
  print ("Plotting the Confusion Matrix")
  labels = list(label_map.values())
  df_cm = pd.DataFrame(cm,index = labels,columns = labels)
  fig = plt.figure()
  res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')
  plt.yticks([0.5,1.5,2.5,3.5,4.5], labels,va='center')
  plt.title('Confusion Matrix - TestData')
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.show()
  plt.close()

plot_confusion_matrix(
        y_test,
        y_pred5
        )

#Load the true pixel data and corresponding labels
X = np.load('pureX.npy')
Y = np.load('dataY.npy')

x_image = np.array( [np.fromstring(df['pixels'][i], dtype=np.uint8, sep=" ") for i in range(len(df))] )

y_image = np.array( [df['emotion'][i] for i in range(len(df)) ])

X_train, X_test, y_train, y_test = train_test_split(x_image, y_image, test_size=0.1,random_state=42)

# @title Method 2: Training Machine Learning Models using Pixel Values

#K Nearest Neighbors Classifier
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 20)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print('K Nearest Neighbors Classifier')
print("Accuracy: ", metrics.accuracy_score(y_test, y_pred))

for i in range(20, 40):
  knn2 = KNeighborsClassifier(n_neighbors = i)
  knn2.fit(X_train, y_train)
  y_pred2 = knn2.predict(X_test)
  print('Accuracy for ', i ,' n_neighbors: ', metrics.accuracy_score(y_test, y_pred2))

#Logistic Regression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train, y_train)
y_pred3 = lr.predict(X_test)
print('Logistic Regression')
print('Accuracy: ', metrics.accuracy_score(y_test, y_pred3))

#Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
print('Random Forest Classifer')
for i in range(100, 250, 10):
  Rf = RandomForestClassifier(i)
  Rf.fit(X_train, y_train)
  y_pred4 = Rf.predict(X_test)
  print('Accuracy for ', i, 'n_estimators : ', metrics.accuracy_score(y_test, y_pred4))

"""# Method 3: Using Neural Networks with Pixel Values"""

# @title Building a Simple Neural Network
perceptron = Sequential()
perceptron.add(Dense(units = 1024, input_shape = (2304, ),kernel_initializer='glorot_normal',activation = 'relu'))
perceptron.add(Dense(units = 512,kernel_initializer='glorot_normal' , activation = 'relu'))
perceptron.add(Dense(units = 5, activation = 'softmax'))

'''perceptron.compile(
    loss='categorical_crossentropy',
    optimizer=SGD(learning_rate=0.001),
    metrics=['accuracy'])'''

# @title Setting up hyperparameters for our model
epochs = 50
batch_size = 16
test_ratio = .1
n_labels = 5

dataX_pixels = np.load('pureX.npy')
dataY_labels = np.load('dataY.npy')

y_onehot = to_categorical(dataY_labels, len(set(dataY_labels)))

X_train, X_test, y_train, y_test = train_test_split(dataX_pixels, y_onehot, test_size=test_ratio, random_state=42)

# Standardize the data
pixel_scaler = StandardScaler()
pixel_scaler.fit(X_train)
X_train = pixel_scaler.transform(X_train)
X_test = pixel_scaler.transform(X_test)

# Compiling the model with SGD optimizer and categorical crossentropy loss
perceptron.compile(loss=categorical_crossentropy, optimizer=SGD(learning_rate=0.001), metrics=['accuracy'])

#Saves the Best Model Based on Val Loss
checkpoint = ModelCheckpoint('best_mlp_model.h5', verbose=1, monitor='val_loss', save_best_only=True,  mode='auto')

#training the model
perceptron_history = perceptron.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,
                            callbacks=[checkpoint], validation_data=(X_test, y_test), shuffle=True)

perceptron_performance = perceptron.evaluate(X_test, y_test, batch_size=64)
plot_graphs(perceptron_history, perceptron_performance);
y_pred_perceptron = perceptron.predict(X_test)
y_pred_perceptron_classes = np.argmax(y_pred_perceptron, axis=1)
y_true = np.argmax(y_test,axis=1)
plot_confusion_matrix(y_true, y_pred_perceptron_classes)

""" # Method 2: Using Neural Networks with Euclidean Distance"""

# @title Building a Simple Neural Network
perceptron_euclid = Sequential()
perceptron_euclid.add(Dense(units = 1024, input_shape = (2278, ),kernel_initializer='glorot_normal',activation = 'relu'))
perceptron_euclid.add(Dense(units = 512,kernel_initializer='glorot_normal' , activation = 'relu'))
perceptron_euclid.add(Dense(units = 5, activation = 'softmax'))

# @title Setting up hyperparameters for the model
epochs = 50
batch_size = 16
test_ratio = .1
n_labels = 5

dataX_distances = np.load('dataX.npy')
dataY_labels = np.load('dataY.npy')

y_onehot = to_categorical(dataY_labels, len(set(dataY_labels)))

X_train, X_test, y_train, y_test = train_test_split(dataX_distances, y_onehot, test_size=test_ratio, random_state=42)

# Standardize the data
pixel_scaler = StandardScaler()
pixel_scaler.fit(X_train)
X_train = pixel_scaler.transform(X_train)
X_test = pixel_scaler.transform(X_test)

X_train.shape

# Compiling the model with SGD optimizer and categorical crossentropy loss
perceptron_euclid.compile(loss=categorical_crossentropy, optimizer=SGD(learning_rate=0.001), metrics=['accuracy'])

#Saves the Best Model Based on Val Loss
checkpoint = ModelCheckpoint('best_mlp_model.keras', verbose=1, monitor='val_loss', save_best_only=True,  mode='auto')

#training the model
perceptron_euclid_history = perceptron_euclid.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,
                            callbacks=[checkpoint], validation_data=(X_test, y_test), shuffle=True)

perceptron_euclid_performance = perceptron_euclid.evaluate(X_test, y_test, batch_size=64)
plot_graphs(perceptron_euclid_history, perceptron_euclid_performance);
y_pred_perceptron = perceptron_euclid.predict(X_test)
y_pred_perceptron_classes = np.argmax(y_pred_perceptron, axis=1)
y_true = np.argmax(y_test,axis=1)
plot_confusion_matrix(y_true, y_pred_perceptron_classes)

"""# Using Convolutional Neural Networks with Image Inputs"""

#same epochs and batch size as above
width, height = 48, 48

print(X_train.shape)
X_train_cnn = X_train.reshape(len(X_train),height,width)
X_test_cnn = X_test.reshape(len(X_test),height,width)

# converting input data to images
print(X_train_cnn.shape)
print(X_test_cnn.shape)

# adding one more dimension for model compatibility
X_train_cnn = np.expand_dims(X_train_cnn,3)
X_test_cnn = np.expand_dims(X_test_cnn,3)

print(X_train_cnn.shape)

cnn_model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape = (48,48,1)),
                                        tf.keras.layers.MaxPool2D(2,2),
                                        tf.keras.layers.Conv2D(64,(3,3),activation='relu',input_shape = (48,48,1)),
                                        tf.keras.layers.MaxPool2D(2,2),
                                        tf.keras.layers.Conv2D(128,(3,3),activation='relu'),
                                        tf.keras.layers.MaxPool2D(2,2),
                                        tf.keras.layers.Conv2D(256,(3,3),activation='relu'),
                                        tf.keras.layers.MaxPool2D(2,2),
                                        tf.keras.layers.Flatten(),
                                        tf.keras.layers.Dense(1000,activation='relu'),
                                        tf.keras.layers.Dense(7,activation = 'softmax')
                                        ])
cnn_model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

from keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

datagen.fit(X_train_cnn)

#Saves the Best Model Based on Val Loss
checkpoint = ModelCheckpoint('best_cnn_model.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')

# compliling the model with adam optimizer and categorical crossentropy loss
cnn_model.compile(loss=categorical_crossentropy, optimizer=Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999), metrics=['accuracy'])

cnn_history = cnn_model.fit(X_train_cnn, y_train, batch_size=batch_size, epochs=epochs, verbose=1,
                            callbacks=[checkpoint], validation_data=(X_test_cnn, y_test), shuffle=True)

cnn_performance = cnn_model.evaluate(X_test_cnn, y_test, batch_size=64)
plot_graphs(cnn_history, cnn_performance);
y_pred_cnn = cnn_model.predict(X_test_cnn)
y_pred_cnn_classes = np.argmax(y_pred_cnn, axis=1)
y_true = np.argmax(y_test,axis=1)
plot_confusion_matrix(y_true, y_pred_cnn_classes)

"""# Method 6: Using the pretrained VGG model for Transfer Learning"""

#@title Building a transfer learning model { display-mode: "form" }

import keras
from keras.models import Sequential
from keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape, Dense, Conv2D, GlobalAveragePooling2D
from keras.applications.vgg16 import VGG16

# load the vgg network that is an 'expert' at 'imagenet' but do not include the FC layers
vgg_expert = VGG16(weights = 'imagenet', include_top = False, input_shape = (48, 48, 3))

# we add the first 12 layers of vgg to our own model vgg_model
vgg_model = Sequential()
vgg_model.add(vgg_expert)

# and then add our own layers on top of it
vgg_model.add(GlobalAveragePooling2D())
vgg_model.add(Dense(1024, activation = 'relu'))
vgg_model.add(Dropout(0.3))
vgg_model.add(Dense(512, activation = 'relu'))
vgg_model.add(Dropout(0.3))
vgg_model.add(Dense(5, activation = 'sigmoid'))

# finally, we build the vgg model and turn it on so we can use it!
vgg_model.compile(loss = 'categorical_crossentropy',
          optimizer = SGD(learning_rate=1e-4, momentum=0.95),
          metrics=['accuracy'])

X_TRAIN = np.array([np.transpose(np.array([X_train_cnn[ix].squeeze() for i in range(3)]), (1,2,0)) for ix in range(len(X_train))])
X_TEST = np.array([np.transpose(np.array([X_test_cnn[ix].squeeze() for i in range(3)]), (1,2,0)) for ix in range(len(X_test))])

#training the model
vgg_history = vgg_model.fit(X_TRAIN, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          callbacks=[checkpoint],
          validation_data=(X_TEST, y_test),
          shuffle=True)

"""# Transfer Learning"""

#@title Run this to build your transfer learning model { display-mode: "form" }

import keras
from keras.models import Sequential
from keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape, Dense, Conv2D, GlobalAveragePooling2D
from keras.wrappers.scikit_learn import KerasClassifier
from keras.applications.vgg16 import VGG16

# load the vgg network that is an 'expert' at 'imagenet' but do not include the FC layers
vgg_expert = VGG16(weights = 'imagenet', include_top = False, input_shape = (48, 48, 3))

# we add the first 12 layers of vgg to our own model vgg_model
vgg_model = Sequential()
vgg_model.add(vgg_expert)

# and then add our own layers on top of it
vgg_model.add(GlobalAveragePooling2D())
vgg_model.add(Dense(1024, activation = 'relu'))
vgg_model.add(Dropout(0.3))
vgg_model.add(Dense(512, activation = 'relu'))
vgg_model.add(Dropout(0.3))
vgg_model.add(Dense(5, activation = 'sigmoid'))

vgg_model.compile(loss = 'categorical_crossentropy',
          optimizer = SGD(lr=1e-4, momentum=0.95),
          metrics=['accuracy'])

X_TRAIN = np.array([np.transpose(np.array([X_train_cnn[ix].squeeze() for i in range(3)]), (1,2,0)) for ix in range(len(X_train))])
X_TEST = np.array([np.transpose(np.array([X_test_cnn[ix].squeeze() for i in range(3)]), (1,2,0)) for ix in range(len(X_test))])

#training the model
vgg_history = vgg_model.fit(X_TRAIN, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          callbacks=[checkpoint],
          validation_data=(X_TEST, y_test),
          shuffle=True)

vgg_performance = vgg_model.evaluate(X_TEST, y_test, batch_size=64)

compare_learning(perceptron_history, perceptron_euclid_history, cnn_history, vgg_history)

cnn_model.save('cnn_model.h5')

"""# Emotion Similarity"""

from google.colab import drive
drive.mount('/content/drive')

import cv2
from google.colab.patches import cv2_imshow
import numpy as np
import dlib

import tensorflow
from tensorflow.keras.models import load_model

emotion_dict = {0: "Angry", 1: "Disgusted", 2: "Fearful", 3: "Happy", 4: "Neutral", 5: "Sad", 6: "Surprised"}

def preprocess(image_path):
    """
    Preprocesses the input image for a facial emotion detection system.

    Steps:
    1. Detect a face using dlib's frontal face detector.
    2. Crop the image to only include the detected face.
    3. Convert the image to grayscale.
    4. Resize the image to 48x48 pixels.

    Parameters:
        input_image (numpy.ndarray): The input image (BGR or grayscale).

    Returns:
        numpy.ndarray: The preprocessed image (grayscale, 48x48 pixels).
    """
    input_image = cv2.imread(image_path)
    detector = dlib.get_frontal_face_detector()

    faces = detector(input_image, 1)

    if len(faces) == 0:
        raise ValueError("No faces detected in the input image.")

    face_rect = faces[0]

    x1, y1, x2, y2 = face_rect.left(), face_rect.top(), face_rect.right(), face_rect.bottom()
    cropped_face = input_image[y1:y2, x1:x2]

    gray_face = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2GRAY) if len(cropped_face.shape) == 3 else cropped_face

    resized_face = cv2.resize(gray_face, (48, 48))

    return resized_face

path_to_model = '/content/drive/MyDrive/FER_models/emotion_model_ck+.h5'
kathakali = load_model(path_to_model)

kathakali.summary()

#image_1 = preprocess('/content/drive/MyDrive/data/photo_face_detection.png')
image_1 = cv2.imread('/content/drive/MyDrive/data/photo_face_detection.png')
image_1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2GRAY)
cv2_imshow(image_1)
image_1 = np.reshape(image_1, (-1, 48, 48, 1))

logits = kathakali.predict(image_1)[0]
print(logits)

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

probabilities = softmax(logits)
print(probabilities)

max_index = np.argmax(probabilities)
emotion_dict[max_index]

image_2 = cv2.imread('/content/drive/MyDrive/data/neutral_udhay.png')
image_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY)
cv2_imshow(image_2)
image_2 = np.reshape(image_2, (-1, 48, 48, 1))

logits_2 = kathakali.predict(image_2)[0]
print(logits_2)

probabilities_2 = softmax(logits_2)
print(probabilities_2)

max_index = np.argmax(probabilities_2)
emotion_dict[max_index]

probabilities_image1 = probabilities
probabilities_image2 = probabilities_2

norm1 = np.linalg.norm(probabilities_image1)
norm2 = np.linalg.norm(probabilities_image2)

cosine_similarity_probabilities = np.dot(probabilities_image1, probabilities_image2) / (norm1 * norm2)

print(cosine_similarity_probabilities)

logits_image1 = logits
logits_image2 = logits_2

norm1 = np.linalg.norm(logits_image1)
norm2 = np.linalg.norm(logits_image2)

cosine_similarity_logits = np.dot(logits_image1, logits_image2) / (norm1 * norm2)

print(cosine_similarity_logits)

"""# Model 2"""

path_to_model = '/content/drive/MyDrive/FER_models/face_recognizer.keras'
kathakali = load_model(path_to_model)

emotion_dict = {0: "Angry", 1: "Disgusted", 2: "Fearful", 3: "Happy", 4: "Neutral", 5: "Sad", 6: "Surprised"}

kathakali.summary()

image_1 = cv2.imread('/content/drive/MyDrive/data/photo_face_detection.png')
image_1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2GRAY)
cv2_imshow(image_1)
image_1 = np.reshape(image_1, (-1, 48, 48, 1))

logits = kathakali.predict(image_1)[0]
print(logits)

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)


probabilities = softmax(logits)
print(probabilities)

max_index = np.argmax(probabilities)
emotion_dict[max_index]

image_2 = cv2.imread('/content/drive/MyDrive/data/surprised.png')
image_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY)
cv2_imshow(image_2)
image_2 = np.reshape(image_2, (-1, 48, 48, 1))

logits_2 = kathakali.predict(image_2)[0]
print(logits_2)

probabilities_2 = softmax(logits_2)
print(probabilities_2)

max_index = np.argmax(probabilities_2)
emotion_dict[max_index]

probabilities_image1 = probabilities
probabilities_image2 = probabilities_2

norm1 = np.linalg.norm(probabilities_image1)
norm2 = np.linalg.norm(probabilities_image2)

cosine_similarity_probabilities = np.dot(probabilities_image1, probabilities_image2) / (norm1 * norm2)

print(cosine_similarity_probabilities)

logits_image1 = logits
logits_image2 = logits_2

norm1 = np.linalg.norm(logits_image1)
norm2 = np.linalg.norm(logits_image2)

cosine_similarity_logits = np.dot(logits_image1, logits_image2) / (norm1 * norm2)

print(cosine_similarity_logits)
